---
title: "Churn Prediction -  PowerCo."
author: "Fasih Atif"
date: "1/4/2021"
output: pdf_document
---

## Background

PowerCo is a major gas and electricity utility that supplies to corporate, SME (Small & Medium Enterprise), and residential customers. The power-liberalization of the energy market in Europe has led to significant customer churn, especially in the SME segment. They have approached me in my capacity as a Data Science Consultant to find out which customers are most likely to churn and what drivers lead to their churn. I met with PowerCo and discussed the various ways we could go about resolving the problems. One of the hypotheses under consideration is that churn is driven by the customers' price sensitivities and that it is possible to predict customers likely to churn using a predictive model. I have received the raw data from the management and have begun working.

```{r load_libraries, include= FALSE}
##########################################
#               Load libraries           #
##########################################

# Set Cran mirror to avoid error
local({r <- getOption("repos")
       r["CRAN"] <- "http://cran.r-project.org" 
       options(repos=r)
})

library(data.table)
library(lubridate)
library(kableExtra)
library(fastDummies)   # To create fummy variables
library(estimatr)
library(scales)        # To take logs of x and y variables
library(faraway)
library(caret)
library(car)           # To calculate VIF
library(ggcorrplot)      # To draw correlation chart
library(outForest)
library(xgboost)
library(Ckmeans.1d.dp) # Required for xgb.ggplot.importance function in xgboost
library(Metrics)       # To calculate AUC
library(ggpubr)
library(knitr)
library(pROC)
library(DataExplorer)
library(huxtable)
library(jtools)
library(mfx)
library(modelsummary)
library(ROCR)
library(dplyr)
library(tidyverse)
```

```{r data_import, include= FALSE}
##########################################
#             Data Import                #
##########################################

# Import CustomerHistory.csv
urlCustomerHistory <- 'https://raw.githubusercontent.com/fasihatif/Data-Analysis-1-2-3/master/Data_Analysis_2/DA2_final_project/data/CustomerDetails.csv'
DfCustomerHistory <- read_csv(urlCustomerHistory)

# Import ChurnOutput.csv
urlChurnOutput <- 'https://raw.githubusercontent.com/fasihatif/Data-Analysis-1-2-3/master/Data_Analysis_2/DA2_final_project/data/ChurnOutput.csv'
DfChurnOutput <- read_csv(urlChurnOutput)
```

## **Understanding Data**

```{r merge_data, include= FALSE}

plot_intro_bar <- plot_intro(DfCustomerHistory, 
           title ="Intro Plot", 
           ggtheme =theme_bw(),
           theme_config=theme(legend.position="bottom"))

# Number of rows in DfCustomerHistory
count(unique(DfCustomerHistory)) #16096

# Number of rows in DfChurnOutput
count(unique(DfChurnOutput)) #16096

# Merge dataframes by id column since equal and has unique ids. We create a df named df_draft where we do all the working for cleaning and feature engineering
df_draft <- merge(DfCustomerHistory,DfChurnOutput, by = 'id')

rm(DfChurnOutput,DfCustomerHistory)

# Get stats of each column
summary(df_draft) # add some analysis regarding consumption and forecasting
```

```{r data_cleaning, include= FALSE, warning = FALSE, message = FALSE}
##########################################
#             Cleaning the Data          #
##########################################

# Number of missing values in each column
na_count <- sapply(df_draft, function(y) sum(length(which(is.na(y)))))
na_count <- data.frame(na_count)

# Number of missing values in percent in each column. 
# We will check which columns have missing values greater than 30% and drop them. We will also ensure that the columns to be dropped dont contain any important information inw hich case we wont drop them.
na_count$na_percent <- sapply(df_draft, function(y) round((sum(length(which(is.na(y))))/length(y))*100.00,2))

# Save the names of columns which have missing values less than 30%
col_30 <- na_count %>% filter(na_percent < 30)
col_30 <- rownames(col_30)

# Remove columns with NA values greater than 30%
df_draft <- df_draft %>% dplyr::select(all_of(col_30))

# Fill in missing dates with median
## Replace NA values in 'date_end' column with median date
df_draft$date_end[is.na(df_draft$date_end)]<-median(df_draft$date_end,na.rm=TRUE)

##Replace NA values in 'date_renewal' column with median date
df_draft$date_renewal[is.na(df_draft$date_renewal)]<-median(df_draft$date_renewal,na.rm=TRUE)

## Replace NA values in 'date_modif_prod' column with median date
df_draft$date_modif_prod[is.na(df_draft$date_modif_prod)]<-median(df_draft$date_modif_prod,na.rm=TRUE)

## Replace NA values in channel sales column with 'null_channel'
df_draft$channel_sales[is.na(df_draft$channel_sales)] <- "null_channel"

# Convert 'has_gas' column from T/F to 1/0
df_draft <- df_draft %>% mutate('has_gas' =  as.numeric(has_gas))
```

```{r feature_engineering, include= FALSE}
##########################################
#            Feature Engineering         #
##########################################

# Create 'contract duration' column and divide by 365 to get yearly values
df_draft <- df_draft %>% mutate('contract_duration' =  as.integer((difftime(date_end,date_activ, unit = "days"))/(365.25/12)))

# Create reference date for calculations. We will take 1st Jan 2020 since it is given as the reference date
ref_date = ymd(20160101)

# No of years since contract went active
df_draft <- df_draft %>% mutate('months_active' =  as.integer((difftime(ref_date,date_activ, unit = "days"))/(365.25/12)))

# No of years left in contract
df_draft <- df_draft %>% mutate('months_end' =  as.integer((difftime(date_end,ref_date, unit = "days"))/(365.25/12)))

# No of years since last modification at reference date
df_draft <- df_draft %>% mutate('months_modif' =  as.integer((difftime(ref_date,date_modif_prod, unit = "days"))/(365.25/12)))

# Number of months since last renewal at reference date
df_draft <- df_draft %>% mutate('months_renewal' =  as.integer((difftime(ref_date,date_renewal, unit = "days"))/(365.25/12)))

# Remove columns with NA values greater than 30%
df <- df_draft %>% dplyr::select(all_of(col_30), contract_duration,has_gas,months_active,months_end,months_modif,months_renewal)
df <- df %>% dplyr::select(-c(date_activ,date_end,date_modif_prod,date_renewal, date_renewal,forecast_cons_year, origin_up))

# Dummy Variable creation for channel_sales and has_gas
## How many dummy columns to make?
# df_backup <- df

## Create dummy variables using the fastdummies library
df <- df %>% dummy_cols(select_columns = c("channel_sales","has_gas"), remove_selected_columns = TRUE)

## Updating column names of dummy variables for easier understanding
df <-  
  rename(df,
    "channel_foos" = channel_sales_foosdfpfkusacimwkcsosbicdxkicaua,
    "channel_usil" = channel_sales_usilxuppasemubllopkaafesmlibmsdf,
    "channel_lmke" = channel_sales_lmkebamcaaclubfxadlmueccxoimlema,
    "channel_ewpa" = channel_sales_ewpakwlliwisiwduibdlfmalxowmwpci,
    "channel_epum" = channel_sales_epumfxlbckeskwekxbiuasklxalciiuu,
    "channel_sddi" = channel_sales_sddiedcslfslkckwlfkdpoeeailfpeds,
    "channel_fixd" = channel_sales_fixdbufsefwooaasfcxdxadsiekoceaa,
    "channel_null" = channel_sales_null_channel
  )

## Remove one of the dummy columns to cater for multicollinearity. We will remove 'channel_null' and 'has_gas_0'
df <- subset(df,select = -c(channel_null,has_gas_0))

```

The raw data consisted of 33 columns and 16096 observations in each column. The data represents the several characteristics of the company that could play an important part in their churning and retention. The data contains variables such as historical usage, dates, forecasted consumption and prices, net/gross margins and churn status. It is often said that 80% of the time is spent in data cleaning and processing while just 20% is spent on running the analytics on the clean data. This saying was applicable in our case as well. There were several data quality issues such as missing values, outliers, incorrect negative values, multicollinearity, and skewed variables. Our dependent variable would be the binary churn variable while the rest of the variables would act as the independent variables. **Figure 1** presents an overview of the raw data.

## **Exploring Data Analysis**

```{r eda, include= FALSE, warning= FALSE, message = FALSE}
# Churn Rate
churn_rate_barchart <- df %>% 
  dplyr::select(churn) %>%
  group_by(churn) %>%
  summarise(percentage = n()) %>%
  mutate(Percent = round(100*percentage/sum(percentage),1)) %>%
  mutate(status = ifelse(churn == 1, "Churned", "Retention")) %>%
  ggplot(aes(x = "Companies", y = Percent, fill= factor(status, levels=c("Churned","Retention")))) +
  geom_bar(stat = "identity") + geom_text(aes(label = Percent), position = position_stack(vjust = .5)) +
  labs(x = '', fill = "Status")


# Function to calculate no of months in the duration between a date column and reference date
months_length <- function(column1, column2){
  
  dataframe <- data.frame(column1, column2)
  
  bar_chart <- dataframe %>%
    group_by(column1, column2) %>%
    summarise(months_count = n()) %>%
    mutate(status = ifelse(column2 == 1, "Churned", "Retention")) %>%
    ggplot(aes(x = column1, y = months_count, fill= factor(status))) +
    geom_bar(stat = "identity") + theme_bw() +
    labs(x = NULL, y = NULL,fill = "Status")
  
  return(bar_chart)
}

# Duration of contract
contract_duration_barchart <- months_length(df$contract_duration,df$churn)

# No of months passed from contract active date to reference date
months_active_barchart <- months_length(df$months_active,df$churn)

# No of months left till end date from reference date
months_end_barchart <- months_length(df$months_end,df$churn)

# No of months left till renewal from reference date
months_renewal_barchart <- months_length(df$months_renewal,df$churn)

# No of months since contract was last modified 
months_modif_barchart <- months_length(df$months_modif,df$churn)

##### CONSUMPTION VARIABLES EXPLORATORY ANALYSIS #####

color <- "cyan3"
consumption_eda <- df %>%
  dplyr::select(cons_12m,cons_gas_12m,cons_last_month,imp_cons) %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) +
  facet_wrap(~key, scales = "free") +
  geom_histogram(fill = color) + theme_bw() +labs(y = NULL, x = NULL)


##### FORECAST VARIABLES EXPLORATORY ANALYSIS #####

forecast_eda <- df %>%
  dplyr::select(forecast_cons_12m,forecast_discount_energy,forecast_meter_rent_12m,forecast_price_energy_p1,forecast_price_energy_p2,forecast_price_pow_p1) %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) +
  facet_wrap(~key, scales = "free") +
  geom_histogram(fill = color) + theme_bw() +labs(y = NULL, x = NULL)

##### MARGIN VARIABLES EXPLORATORY ANALYSIS #####

margin_eda <- df %>%
  dplyr::select(margin_gross_pow_ele,margin_net_pow_ele,net_margin) %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) +
  facet_wrap(~key, scales = "free") +
  geom_histogram(fill = color) + theme_bw() +labs(y = NULL, x = NULL)

##### OTHER VARIABLES EXPLORATORY ANALYSIS #####

other_eda <- df %>%
  dplyr::select(nb_prod_act,num_years_antig,pow_max) %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) +
  facet_wrap(~key, scales = "free") +
  geom_histogram(fill = color) + theme_bw() +labs(y = NULL, x = NULL)

```

```{r loess, include = FALSE, warning = FALSE, message = FALSE}

# Non Parametric Charts
# cons_12m
cons_12m_sc <- ggplot(df , aes(x = cons_12m, y = churn)) +
  geom_point() +
  geom_smooth(method="loess")+
  labs(x = "cons_12m",y = "Churn", title = "Level - Level for churn~cons_12m") 

ln_cons_12m_sc <- ggplot(df , aes(x = cons_12m, y = churn)) +
  geom_point() +
  geom_smooth(method="loess")+
  labs(x = "cons_12m, ln scale",y = "Churn", title = "Level - log for churn~ln_cons_12m") + 
  scale_x_continuous(trans = log_trans())

# cons_last_month
cons_last_month_sc <- ggplot(df , aes(x = cons_last_month, y = churn)) +
  geom_point() +
  geom_smooth(method="loess")+
  labs(x = "cons_last_month",y = "Churn", title = "Level - Level for churn~cons_last_month") 

ln_cons_last_month_sc <- ggplot(df , aes(x = cons_last_month, y = churn)) +
  geom_point() +
  geom_smooth(method="loess")+
  labs(x = "cons_last_month, ln scale",y = "Churn", title = "Level - log for churn~ln_cons_last_month") + 
  scale_x_continuous(trans = log_trans())

# imp_cons
imp_cons_sc <- ggplot(df , aes(x = imp_cons, y = churn)) +
  geom_point() +
  geom_smooth(method="loess") +
  labs(x = "imp_cons",y = "Churn", title = "Level - Level for churn~imp_cons") 

ln_imp_cons_sc <- ggplot(df , aes(x = imp_cons, y = churn)) +
  geom_point() +
  geom_smooth(method="loess")+
  labs(x = "imp_cons, ln scale",y = "Churn", title = "Level - log for churn~ln_imp_cons") + 
  scale_x_continuous(trans = log_trans())

# cons_gas_12m
cons_gas_12m_sc <- ggplot(df , aes(x = cons_gas_12m, y = churn)) +
  geom_point() +
  geom_smooth(method="loess") +
  labs(x = "cons_gas_12m",y = "Churn", title = "Level - Level for churn~cons_gas_12m") 

ln_cons_gas_12m_sc <- ggplot(df , aes(x = cons_gas_12m, y = churn)) +
  geom_point() +
  geom_smooth(method="loess")+
  labs(x = "cons_gas_12m, ln scale",y = "Churn", title = "Level - log for churn~ln_cons_gas_12m") + 
  scale_x_continuous(trans = log_trans())

# forecast_cons_12m
forecast_cons_12m_sc <- ggplot(df , aes(x = forecast_cons_12m, y = churn)) +
  geom_point() +
  geom_smooth(method="loess") +
  labs(x = "forecast_cons_12m",y = "Churn", title = "Level - Level for churn~forecast_cons_12m") 

ln_forecast_cons_12m_sc <- ggplot(df , aes(x = cons_gas_12m, y = churn)) +
  geom_point() +
  geom_smooth(method="loess")+
  labs(x = "forecast_cons_12m, ln scale",y = "Churn", title = "Level - log for churn~ln_forecast_cons_12m") + 
  scale_x_continuous(trans = log_trans())

# forecast_meter_rent_12m
forecast_meter_rent_12m_sc <- ggplot(df , aes(x = forecast_meter_rent_12m, y = churn)) +
  geom_point() +
  geom_smooth(method="loess") +
  labs(x = "forecast_meter_rent_12m",y = "Churn", title = "Level - Level for churn~forecast_meter_rent_12m") 

ln_forecast_meter_rent_12m_sc <- ggplot(df , aes(x = forecast_meter_rent_12m, y = churn)) +
  geom_point() +
  geom_smooth(method="loess")+
  labs(x = "forecast_meter_rent_12m, ln scale",y = "Churn", title = "Level - log for churn~ln_forecast_meter_rent_12m") + 
  scale_x_continuous(trans = log_trans())
```

```{r further_data_cleaning, include = FALSE,warning = FALSE, message = FALSE}

# Subsetting important variables for outlier treatment
# df <- subset(df,select = -c(id,cons_12m,cons_gas_12m,cons_last_month,imp_cons,forecast_cons_12m,forecast_meter_rent_12m))
df_other <- df %>% dplyr::select(-c(forecast_price_energy_p1,forecast_price_energy_p2,forecast_price_pow_p1,margin_gross_pow_ele,margin_net_pow_ele,net_margin,pow_max,cons_12m,cons_gas_12m,cons_last_month,imp_cons,forecast_cons_12m,forecast_meter_rent_12m))
df <- df %>% dplyr::select(c(forecast_price_energy_p1,forecast_price_energy_p2,forecast_price_pow_p1,margin_gross_pow_ele,margin_net_pow_ele,net_margin,pow_max,cons_12m,cons_gas_12m,cons_last_month,imp_cons,forecast_cons_12m,forecast_meter_rent_12m))

# Detecting outliers and replacing them with mean
remove_outliers <- function(column_name){
  outliers <- boxplot(column_name, plot=FALSE)$out
  if(length(outliers) == 0){ column_name  <- column_name} else{
    column_name[column_name %in% outliers] = mean(column_name,na.rm = TRUE); column_name
  }
}

# Application of outlier function
df <- data.frame(sapply(df, remove_outliers))

# Join the treated dataset with rest of variables
df <- cbind(df_other,df)

# Continue on with df dataframe and treat it for missing values
# Remove all NA values and replace with mean

df_id <- df %>% dplyr::select(id)
df <- df %>% dplyr::select(-id)

rm(df_other)
```

```{r transformation, include=FALSE, message=FALSE, warning=FALSE}
##########################################
#        Transformation of data          #
##########################################
##### Transformation of Consumption variables #####

# Consumption variables are right skewed as we saw from the histograms. To make them normally distributed, we will
# take log of these variables. However, these 4 variables include negative and zero values for which log cant be taken.So 
# we will convert negative values to NaN and add a constant 1 to these variables as well

# Set negative values as Na as log cant be taken for negative values
df <- df %>% mutate(cons_12m = replace(cons_12m, which(cons_12m < 0), NA))
df <- df %>% mutate(cons_gas_12m = replace(cons_gas_12m, which(cons_gas_12m < 0), NA))
df <- df %>% mutate(cons_last_month = replace(cons_last_month, which(cons_last_month < 0), NA))
df <- df %>% mutate(imp_cons = replace(imp_cons , which(imp_cons  < 0), NA))

# Add constant 1 to the variables and then take log since some values are zero
df <- df %>% mutate( ln_cons_12m = log( cons_12m + 1 ),
                     ln_cons_gas_12m = log( cons_gas_12m + 1),
                     ln_cons_last_month = log(cons_last_month + 1),
                     ln_imp_cons = log(imp_cons + 1)) 


##### Transformation of Forecast variables #####

# Set negative values as NaN as log cant be taken for negative values
df <- df %>% mutate(forecast_cons_12m = replace(forecast_cons_12m, which(forecast_cons_12m < 0), NA))
df <- df %>% mutate(forecast_meter_rent_12m = replace(forecast_meter_rent_12m, which(forecast_meter_rent_12m < 0), NA))


# Add constant 1 to the variables and then take log
df <- df %>% mutate( ln_forecast_cons_12m = log(forecast_cons_12m + 1),
                     ln_forecast_meter_rent_12m = log(forecast_meter_rent_12m + 1)) 

# Fill missing values with mean
df <- data.frame(sapply(df, function(x) replace(x, is.na(x), mean(x, na.rm = TRUE))))
# backup2 <- df

# Now check for the distribution of the transformed variables
transformed_eda <- df %>%
  dplyr::select(ln_cons_12m,ln_cons_gas_12m,ln_cons_last_month,ln_imp_cons,ln_forecast_cons_12m,ln_forecast_meter_rent_12m) %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) +
  facet_wrap(~key, scales = "free") +
  geom_histogram(fill = color) + theme_bw() +labs(y = NULL, x = NULL)

# Remove original columns which have been transformed
df <- df %>% dplyr::select(-c(cons_12m,cons_gas_12m,cons_last_month,imp_cons,forecast_cons_12m,forecast_meter_rent_12m))

```

```{r correlation_matrix, include = FALSE, warning = FALSE, message = FALSE}

##### Correlation Matrix #####
cor1 <- cor(df, use = "pairwise.complete.obs")
cor_matrix <- ggcorrplot::ggcorrplot(cor1, method = "square",lab = TRUE, type = "lower", lab_size = 2.5, digits = 2,ggtheme = theme_bw)

# From the correlation matrix, we can see that 'contract_duration'& 'month_activ','num_years_antig' & 'months_end', 'margin_gross_power_ele' & 'margin_net_power_ele' have the highest correlation
# Calculate Variance Inflation Factor using the 'car' package
lm_model <- lm(churn ~ ., data = df)
vif <- data.frame(car::vif(lm_model))
vif <- rename(vif, VIF = car..vif.lm_model.)

# From the VIF we can confirm that the correlations are very high and we can drop one of the variables from each correlation pair.

df <- subset(df, select = -c(contract_duration,num_years_antig, margin_gross_pow_ele,ln_cons_gas_12m))

```

The missing data (**Table 1**) was very small for majority of the variables, so i was able to easily replace the missing values with mean and median approximations. Any column that had more than 30% missing observations were removed. I then moved on to conducted Feature Engineering to draw more useful variables from our existing ones. I converted dates into monthly durations to make them more useful (**Figure 2**). After getting done with our initial data cleaning and feature Engineering, i checked the distributions of the variables through histograms (**Figure 1-9**).

Our exploratory data analysis shows that around 10% of the of total customers have churned (**Figure 3**). All Consumption related variables (cons\_12m ,cons\_last\_month,imp\_cons,cons\_gas\_12m ) and 2 of the forecast variables (forecast\_cons\_12m,forecast\_meter\_rent\_12m) are extremely right skewed (**Figure 4,5**). The values on the higher end and lower ends of the distribution are potential outliers. Next we checked a non-parametric estimator loess smoother to have a general idea about the functional form between a variable and churn. We will specifically focus on those variables who had skewed distributions (Figure ). As expected, the functional forms are very non linear and hence we took log of the 6 variables (4 consumption and 2 forecast variables) to give the loess line a more linear shape (**Figure 6,7**). Before i took the logs, i converted the negative values to NA values in the consumption related variables. Firstly it will allows us to take logs and secondly these negative values seem to be corrupted data. The energy consumption variables negative state means that the customers were now returning/creating the energy instead of buying from PowerCo. The missing values were then umputed with mean values. The transformation of the log variables can be seen in (**Figure 9**). I used boxplots to tell us more about the outliers and what their values are. We then removed these values and replaced them with the mean of the respective columns values excluding the outliers.

**Multicollinearity and Dummy Variables**

When you have two independent variables that are very highly correlated, you definitely should remove one of them because you run into the multicollinearity conundrum and your regression model's regression coefficients related to the two highly correlated variables will be unreliable. I checked for multicollinearity between the explanatory variables. I drew a correlated matrix (**Figure 10**) to observe which pair of variables were highly correlated. Once I noticed some highly correlated pairs, we calculated the Variance Inflation Factor (VIF) (**Table 2**) to confirm the correlation and remove one of the variable from the correlated pairs.This was done to reduce multicollinearity. Categorical columns ('channel\_sales' and 'has\_gas') were converted into dummy columns and the reference columns were removed.

**Machine Learning, Probabilities, and Predictions**

```{r split_data, include = FALSE, warning= FALSE, message = FALSE}
df <- df %>% mutate(churn = as.factor(churn))

##### SPLIT DATA INTO TEST/TRAIN DATASET #####
set.seed(1111)
intrain<- createDataPartition(df$churn,p=0.75,list=FALSE)
train <- df[intrain,]
test <- df[-intrain,]

```

```{r sum_stat, include = FALSE, warning= FALSE,message = FALSE}
sum_stat <- function( df , var_names , stats , num_obs = TRUE ){
  k <- length( var_names )
  built_in_stat <- c('mean','median','mode','min','max','1st_qu.','3rd_qu',
                     'sd','var','range','iqr')
  do_stat <- intersect( stats , built_in_stat )
  if ( is_empty(do_stat) ){
    stop('Error, no such statistics is implemented! Choose from: mean,median,min,max,1st_qu.,3rd_qu')
  }
  # By default add the number of missing observations and the number of used observations
  m <- length( do_stat )
  if ( num_obs ){
    do_stat <- c( do_stat , "# missing", "# used obs")
  }
  # Create tibble for output
  sum_stat <- as_tibble( matrix( 0 , nrow = m , ncol = k ) , name_repair = "unique" )
  for ( j in 1 : k ) {
    # Get the data for the j'th variable
    var_j <- df[ var_names[ j ] ]
    if ( num_obs ){
      # Count the missing values and add to statistics
      sum_stat[ m + 1 , j ] <- as.integer( sum( is.na( var_j ) ) )
      # Count observations used
      sum_stat[ m + 2 , j ] <- as.integer( sum( !is.na( var_j ) ) )
    }
    # Remove missing values
    var_j <- var_j[ !is.na( var_j ) ]
    # Name the sum_stat's column
    colnames( sum_stat )[ j ] <- var_names[ j ]
    for ( i in 1 : m ) {
      # Central tendency
      if (do_stat[ i ] == "mean"){
        sum_stat[[i,j]] <- mean( var_j )
      } else if (do_stat[ i ] == "median"){
        sum_stat[i,j] <- median( var_j )
      } else if (do_stat[ i ] == "mode"){
        sum_stat[i,j] <- mode( var_j )
      } 
      # Support
      else if (do_stat[ i ] == "min"){
        sum_stat[i,j] <- min( var_j )
      } else if (do_stat[ i ] == "max"){
        sum_stat[i,j] <- max( var_j )
      } 
      # Quartiles
      else if (do_stat[ i ] == "1st_qu."){
        sum_stat[i,j] <- quantile( var_j , probs = 0.25 )
      } else if (do_stat[ i ] == "3rd_qu"){
        sum_stat[i,j] <- quantile( var_j , probs = 0.75)
      } 
      # Dispersion
      else if (do_stat[ i ] == "sd"){
        sum_stat[i,j] <- sd( var_j )
      } else if (do_stat[ i ] == "var"){
        sum_stat[i,j] <- var( var_j )
      } else if (do_stat[ i ] == "range"){
        sum_stat[i,j] <- max( var_j ) - min( var_j )
      } else if (do_stat[ i ] == "iqr"){
        sum_stat[i,j] <- quantile( var_j , probs = 0.75) - quantile( var_j , probs = 0.25)
      } 
    }
  }
  # Finally add a column which contains the requested statistics and relocate to first position
  sum_stat <- sum_stat %>% 
    mutate( statistics = do_stat ) %>% 
    relocate( statistics )
  
  return( sum_stat )
}
```

I devised the following equation that will be used in the models:

![](images/equation.jpg)

## **Linear probability Model**

Now that our data is ready for analysis we will start off with same basic probability models such as the Linear Probability Model.We split the data into train and test groups in a 75/25 ratio. The produced coefficients for this model can been see in (**Table 3**). We used the model to predict on the test set. The probability distribution is centered around 1.1 (**Figure 11**). Probability values are between 0 and 1 but in our model, half of our probabilities are greater than 1. This is a drawback of the model. Nevertheless, we will try to study the characteristics of the top 1% customers who have the highest and lowest probability of leaving. The results are shown in (**Table 4**). The AIC for the model was 4893.13 and BIC was 5092.895.

```{r lpm, include = FALSE, warning= FALSE, message = FALSE}
##########################################
#         Linear Probability Model       #
##########################################

train_lpm <- train
test_lpm <- test

train_lpm$churn <- as.numeric(train_lpm$churn)
test_lpm$churn <- as.numeric(test_lpm$churn)

##### SPLIT DATA INTO TEST/TRAIN DATASET #####

lp_model <-lm(churn ~ ., data = train_lpm)
summary(lp_model)
print(lp_model)

# Check predicted probabilities: is there any interesting values?
# predicted probabilities
test_lpm$pred_lpm <- predict(lp_model,test_lpm )
# Make a descriptive summary of the predictions!
summary(test_lpm$pred_lpm)

# Show the predicted probabilities' distribution (ggplot)
lpm_pred_prob <- ggplot( test_lpm , aes( x = pred_lpm ) ) +
  geom_histogram(fill = 'cyan3', color = 'black') + theme_bw() + labs(title = "Predicted Probabilities -LPM Model")

test_lpm <- test_lpm %>% 
  mutate( q100_pred_lpm = ntile(pred_lpm, 100) )

# Top 1%
t1 <- sum_stat( subset(test_lpm , q100_pred_lpm==100 ) , 
                c('forecast_price_pow_p1', 'ln_forecast_cons_12m',
                  'pow_max','margin_net_pow_ele','net_margin'),
                c('mean','median','sd'),
                num_obs = F )
t1

# Bottom 1%
b1 <- sum_stat( subset( test_lpm , q100_pred_lpm==1 ) , 
                c('forecast_price_pow_p1', 'ln_forecast_cons_12m',
                  'pow_max','margin_net_pow_ele','net_margin' ),
                c('mean','median','sd'),
                num_obs = F )
b1

AIC(lp_model)
BIC(lp_model)


```

## **Logistic Regression**

```{r glm, include = FALSE, warning= FALSE, message = FALSE}

#########################################
##### LOGISTIC REGRESSION VIA GLM()  ####
#########################################

glm_train <- train
glm_test <- test

# Logit Model 1
logit_model1 <- glm(as.factor(churn) ~ ., data = glm_train, family = binomial("logit"))

# Logit Model 2
logit_model2 <- glm(as.factor(churn)~. -channel_sddi -channel_epum -net_margin -ln_forecast_meter_rent_12m -nb_prod_act -forecast_price_pow_p1 -months_end, data = glm_train, family = binomial("logit"))

# Probit Model 1
probit_model <- glm(as.factor(churn)~., data = glm_train, family = binomial("probit"))
BIC(logit_model1)

# Model  Prediction with Logit
pred_type_test_l <- predict(logit_model2, newdata = glm_test, type = "response")
pred_type_logit <- pred_type_test_l
glm_test <- cbind(pred_type_test_l,test)
pred_type_test_l <- ifelse(pred_type_test_l > 0.5, 1, 0)

# Marginal Differences Logit
# Calculate logit marginal differences
logit_marg <- logitmfx(as.factor(churn)~., data=glm_train, atmean=FALSE, robust = T)
print(logit_marg)

#Model Prediction with Probit
pred_type_logit  <- predict(logit_model2, newdata = glm_test, type = "response")
glm_test <- cbind(pred_type_logit,test)

# Calculate probit marginal differences
probit_marg <- probitmfx(as.factor(churn)~., data=glm_train, atmean=FALSE, robust = T)
print( probit_marg )


#Save models
w_dir <- 'C:/Users/abc/OneDrive/Business_Analytics/Data-Analysis-1-2-3/Data_Analysis_2/DA2_final_project/out/'
cm <- c('(Intercept)' = 'Constant')
pmodels <- list(logit_model1, logit_model2,logit_marg, probit_model, probit_marg)
msummary( pmodels ,
          fmt="%.3f",
          gof_omit = 'DF|Deviance|Log.Lik.|F|R2 Adj.|AIC|BIC|R2|PseudoR2',
          stars=c('*' = .05, '**' = .01),
          coef_rename = cm,
          coef_omit = 'as.factor(churn)*',
          output = paste0(w_dir,"prob_models_coeff.html")
)


# Confusion Matrix
cm_glm <- confusionMatrix(as.factor(pred_type_test_l),as.factor(glm_test$churn))


# Producing ROC curve of model
#pred_response <- predict(glm_model1, newdata = test, type = "response")
predictFull <- prediction(pred_type_test_l,as.factor(glm_test$churn))

# Plot AUC
auc_roc <- performance(predictFull, measure = 'tpr',x.measure = 'fpr')
roc_plot <- plot(auc_roc, col = "blue")

auc_score_glm <- auc(glm_test$churn, pred_type_test_l) #0.6221063
rmse_score_glm <- rmse(glm_test$churn, pred_type_test_l) # 0.2994171
```

Logistic Regression belongs to the family of generalized linear models. It is a binary classification algorithm used when the response variable is dichotomous (1 or 0). Inherently, it returns the set of probabilities of target class. But, we can also obtain response labels using a probability threshold value.

Logistic Regression with Logit/Probit models will suit our research better as they limit the probability between 0 and 1. We first ran a logit model with all variables and looked at the results. The most insignificant variables were removed and a second logit model was performed. The second logit model performed slightly better with lesser coefficients. We calculated the marginal difference for logit model. Then we ran Probit model and Probit marginal difference models to see if it performed better than logit. The coefficients of the 4 models can be compared in (**Table 5**).

The AIC for logit model is 7593.75 and BIC was 7786.119. The AIC for Probit model was 7595.39 and BIC was 7787.75. The logit model performed better overall.

The confusion matrix is shown below:

![](C:/Users/abc/AppData/Local/RStudio/tmp/paste-1CD9D45B.png){width="220"}

It shows that our model predicted correctly with an accuracy of 90.1%. We also drew an ROC curve (**Figure 12**) and computed the AUC validated which turned out to be 0.5. The model can be greatly improved if more work is done on this.

## XGBoost Model

```{r xgboost, include = FALSE, message = FALSE, warning = FALSE}

set.seed(2018)
#intrain<- createDataPartition(df_xgb$churn,p=0.75,list=FALSE)
ml_train <- train
ml_test <-  test

labels <- ml_train$churn
ts_label <- ml_test$churn

labels <- as.numeric(as.character(labels))
ts_label <- as.numeric(as.character(ts_label))

#XGBoost takes matrix for data hence we convert dataframe to matrix
# df_xgb_backup<- df_xgb
xgb_train <- ml_train %>% dplyr::select(-churn)
xgb_train <- xgb.DMatrix(data = as.matrix(xgb_train),label = labels)

xgb_test <- ml_test %>% dplyr::select(-churn)
xgb_test <- xgb.DMatrix(data = as.matrix(xgb_test),label = ts_label)

#default parameters
xgb_params <- list(booster = "gbtree", 
                   objective = "binary:logistic", 
                   eta=0.3, gamma=0, 
                   max_depth=6, 
                   min_child_weight=1, 
                   subsample=1, 
                   colsample_bytree=1)

# Calculate the best nround for this model. In addition, this function also returns CV error, which is an estimate of test error.
set.seed(2018)
xgbcv <- xgb.cv( params = xgb_params, data = xgb_train, nrounds = 500, nfold = 5, showsd = T, stratified = T, print_every_n = 10, early_stopping_rounds = 20, maximize = F, missing = NA)

# Iteration 47 gave lowest test error

elog <- as.data.frame(xgbcv$evaluation_log)
nround <- which.min(elog$test_error_mean)
##best iteration = 47
## The model returned lowest error at the 47th (nround) iteration.
# CV accuracy is 1-0.0968 = 90.32%

#first default - model training
xgb_model <- xgb.train(params = xgb_params, 
                        data = xgb_train, 
                        nrounds = nround, 
                        watchlist = list(train=xgb_train,test=xgb_test), 
                        print_every_n = 10, early_stop_round = 10, 
                        maximize = F , 
                        eval_metric = "error")

summary(xgb_model)
#model prediction
xgbpred <- predict(xgb_model,xgb_test)

summary(xgbpred)

# The objective function binary:logistic returns output probabilities rather than labels. To convert it, we need to 
# manually use a cutoff value. As seen above, I've used 0.5 as my cutoff value for predictions. We can calculate our model's
# accuracy using confusionMatrix() function from caret package.

#confusion matrix
xgbpred <- ifelse(xgbpred > 0.5,1,0)
cm_xgb <- confusionMatrix (as.factor(xgbpred), as.factor(ts_label))
#Accuracy - 86.54%` 

#view variable importance plot
mat <- xgb.importance (feature_names = colnames(ml_train),model = xgb_model)

# The ggplot-backend method also performs 1-D clustering of the importance values, with bar colors 
# corresponding to different clusters that have somewhat similar importance values.
xgb_feature_plot <- xgb.ggplot.importance (importance_matrix = mat)

library(Metrics) 
roc_test <- roc(ts_label, xgbpred, algorithm = 2)
roc_xgb <- plot(roc_test) 
auc_xgb <- auc(ts_label, xgbpred) #0.5684948
rmse_xgb <- rmse(ts_label, xgbpred) # 0.2953
```

It is known for its good performance as compared to all other machine learning algorithms and has been a winner in many Data Science competitions. I wanted to run and try a bigger complex model to see if they perform any better or is simpler the better? I set the parameters for 10 k folds and 500 rounds and used cross validation to check on which round we would get the lowest test mean error. The model stopped on round 47 and and achieved an accuracy of 90.32%. I used 47 rounds in our main XGBoost training model and predicted our test set. The accuracy on our test set was approximately 90.73\$ with confidence intervals of 89 and 91%. The confusion matrix for the XGboost model is below:

.![](C:/Users/abc/AppData/Local/RStudio/tmp/paste-DE476C08.png){width="188" height="70"}

We can see that the model correctly predicted that a majority of the customers wont churn and in reality they didnt. This shows that our model has a very high sensitivity of 99% based on our test set.

Its not exactly easy to interpret information from XGBoost model without processing it further but we can use the data to show which are the most influential variables:

```{r, echo = FALSE, warning= FALSE, message = FALSE, fig.align="center"}
xgb_feature_plot

```

The chart shows that forecast\_price\_pow\_p1 , ln\_forecast\_cons\_12m, pow\_max are some of the most important variables that the management should look at when dealing with customers.

The ROC (**Figure 13**) of the XGboost model fares better than the logit model as the AUC comes out to be 0.5684.

## **Conclusion**

PowerCo appraoched me to help them predict which customers are most likely to leave. I did some exploratory data analysis and transformed the variables. I then conducted some machine learning classification and analysed probabilities. Both Logit and XGboost models performed very well and had accuracy above 90%. However, the AUC , AIC, and BIC scores were beter in XGBoost model. I would recommend using both logistic and XGboost models. Logistic has comparatively easier interpretations and easier to use.

\newpage

### **APPENDIX A**

**Table 1 - Missing Values**

```{r missing_values, echo=FALSE, message=FALSE, warning=FALSE}
library(knitr)
kable(na_count)
```

\newpage

**Table 2 - Variance Inflation Factor (VIF)** \\

```{r vif,echo=FALSE, warning = FALSE, message = FALSE, fig.align="center"}
library(knitr)
kable(vif)
```

\newpage

**Table 3 - Linear Probability Model Coefficients**

![](images/lpm.PNG)

\newpage

**Table 4 - Top and Bottom 1% companies**

```{r, echo = FALSE,fig.align="left"}
kable(t1)
kable(b1)

```

\newpage

**TABLE 5 - Logistic Regression Model Summaries**

(From left to right: logit\_model\_2, logit\_marg, probit\_model\_1,probit\_marg)

![](C:/Users/abc/AppData/Local/RStudio/tmp/paste-93152B5F.png)

\newpage

## **APPENDIX B**

**Figure: 1 - Data Overview**

![](C:/Users/abc/AppData/Local/RStudio/tmp/paste-98F2FDB6.png)

**Figure: 2 - Monthly Duration charts**

```{r months_active_end, echo=FALSE, fig.height=5, fig.width=8, message=FALSE, warning=FALSE, fig.align="center"}
ggarrange1 <- ggarrange(months_active_barchart,months_end_barchart,months_renewal_barchart,months_modif_barchart,
          nrow = 2, ncol = 2,
          common.legend = TRUE,
          labels = c("Months Active", "Months End", "Months Renewal", "Months_modif"),
          vjust = c(-0.5,-0.5,0.4,0.4),
          hjust = c(-1.8,-2.4,-1.5,-2.0),
          font.label = list(size = 10))

annotate_figure(ggarrange1,
                bottom = text_grob("No of Months", color = "black",
                                   hjust = 4.3, x = 1, size = 11, face = "bold"),
                left = text_grob("No of Companies", color = "black", rot = 90, face = "bold",size = 11))
```

**Figure: 3 - Churn Rate**

```{r churn_rate, echo = FALSE, warning= FALSE,message= FALSE, fig.width=8,fig.height=5, fig.align="center"}
print(churn_rate_barchart)
```

\newpage

**Figure: 4 - Consumption Variables Exploration**

```{r consumption_eda, echo = FALSE, warning = FALSE, message = FALSE, fig.align="center",fig.height= 4}
##### CONSUMPTION VARIABLES EXPLORATORY ANALYSIS #####
annotate_figure(consumption_eda,
                bottom = text_grob("Consumption", color = "black",
                                   hjust =3.9, x = 1, size = 11, face = "bold"),
                left = text_grob("No of Companies", color = "black", rot = 90, face = "bold",size = 11))


```

**Figure: 5 - Forecast Variables Exploration**

```{r forecast_eda, echo=FALSE, message=FALSE, warning=FALSE, fig.align="center",fig.height= 4}
annotate_figure(forecast_eda,
                bottom = text_grob("Dollars ($)", color = "black",
                                   hjust = 5, x = 1, size = 11, face = "bold"),
                left = text_grob("No of Companies", color = "black", rot = 90, face = "bold",size = 11))
```

\newpage

**Figure: 6 - Margin Variables Exploration**

```{r margin_eda, echo = FALSE, message=FALSE, warning=FALSE, fig.align="center",fig.height= 3}
annotate_figure(margin_eda,
                bottom = text_grob("Dollars ($)", color = "black",
                                   hjust = 4.75, x = 1, size = 11, face = "bold"),
                left = text_grob("No of Companies", color = "black", rot = 90, face = "bold",size = 11))
```

**Figure: 7 - Other Variables Exploration**

```{r other_eda, echo = FALSE, message=FALSE, warning=FALSE, fig.align="center",fig.height= 3}
annotate_figure(other_eda,
                bottom = text_grob("Value", color = "black",
                                   hjust = 8.8, x = 1, size = 11, face = "bold"),
                left = text_grob("No of Companies", color = "black", rot = 90, face = "bold",size = 11))
```

\newpage

**Figure: 8 - Loess/Scatterplot of Variables**

```{r, fig.align="center", fig.height=3, message=FALSE, warning=FALSE, echo=FALSE}
ggpubr::ggarrange(cons_12m_sc,ln_cons_12m_sc, nrow = 1, ncol = 2)
ggpubr::ggarrange(cons_last_month_sc,ln_cons_last_month_sc, nrow = 1, ncol = 2)
ggpubr::ggarrange(imp_cons_sc,ln_imp_cons_sc, nrow = 1, ncol = 2)
ggpubr::ggarrange(cons_gas_12m_sc,ln_cons_gas_12m_sc, nrow = 1, ncol = 2)
ggpubr::ggarrange(forecast_cons_12m_sc,ln_forecast_cons_12m_sc, nrow = 1, ncol = 2)
 


#ln_cons_last_month_sc,imp_cons_sc,ln_imp_cons_sc,cons_gas_12m_sc,ln_cons_gas_12m_sc,forecast_cons_12m_sc, ln_forecast_cons_12m_sc,forecast_meter_rent_12m_sc,ln_forecast_meter_rent_12m_sc, nrow = 6, ncol = 2)

```

**Figure: 9 - Transformed Variables Exploration**

```{r echo=FALSE, fig.align="center", fig.height=4, message=FALSE, warning=FALSE}
annotate_figure(transformed_eda,
                bottom = text_grob("Value", color = "black",
                                   hjust = 8.8, x = 1, size = 11, face = "bold"),
                left = text_grob("No of Companies", color = "black", rot = 90, face = "bold",size = 11))
```

\newpage

**Figure 10 - Correlation Matrix**

```{r correlation_matrix_chart,echo = FALSE, warning = FALSE, message = FALSE, fig.height=11,fig.width=11, fig.align="center"}
library(ggpubr)
cor_matrix + rotate_x_text(angle = 90)

```

\newpage

**Figure 11 - Probability Distribution**

```{r, echo = FALSE,message=FALSE, warning=FALSE}
lpm_pred_prob

```

**Figure 12 - ROC Plot**

```{r, ROC,echo = FALSE, warning = FALSE, message = FALSE}
plot(auc_roc, col = "blue")

```

**Figure 13 - ROC Plot XGBoost**

```{r, ROC_XG,echo = FALSE, warning = FALSE, message = FALSE}
plot(roc_test) 

```
