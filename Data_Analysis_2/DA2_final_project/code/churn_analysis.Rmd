---
title: "Churn Prediction -  PowerCo."
author: "Fasih Atif"
output: 
  pdf_document: 
    fig_height: 4
header-includes:
  - \usepackage{titling}
  - \setlength{\droptitle}{-5em}
---

Define what is your (broad) research question! What you want to learn from the data or what you want to use it? (a) What is the main aim: prediction or causality?

```{r load_libraries, include= FALSE}
##########################################
#               Load libraries           #
##########################################
library(tidyverse)
library(data.table)
library(lubridate)
library(fastDummies)   # To create fummy variables
library(estimatr)
library(scales)        # To take logs of x and y variables
library(faraway)
library(caret)
library(car)           # To calculate VIF
library(ggcorrplot)      # To draw correlation chart
library(outForest)
library(xgboost)
library(Ckmeans.1d.dp) # Required for xgb.ggplot.importance function in xgboost
library(randomForest)
library(Metrics)       # To calculate AUC
library(ggpubr)
library(knitr)
library(pROC)
library(skimr)
library(DataExplorer)
library(dplyr)
library(stargazer)
library(mfx)
library(modelsummary)
library(ROCR)
```

```{r data_import, include= FALSE}
##########################################
#             Data Import                #
##########################################

# Import CustomerHistory.csv
urlCustomerHistory <- 'https://raw.githubusercontent.com/fasihatif/Data-Analysis-1-2-3/master/Data_Analysis_2/DA2_final_project/data/CustomerDetails.csv'
DfCustomerHistory <- read_csv(urlCustomerHistory)

# Import ChurnOutput.csv
urlChurnOutput <- 'https://raw.githubusercontent.com/fasihatif/Data-Analysis-1-2-3/master/Data_Analysis_2/DA2_final_project/data/ChurnOutput.csv'
DfChurnOutput <- read_csv(urlChurnOutput)
```

## **Understanding our Data**

```{r merge_data, include= FALSE}

plot_intro_bar <- plot_intro(DfCustomerHistory, 
           title ="Intro Plot", 
           ggtheme =theme_bw(),
           theme_config=theme(legend.position="bottom"))

# Number of rows in DfCustomerHistory
count(unique(DfCustomerHistory)) #16096

# Number of rows in DfChurnOutput
count(unique(DfChurnOutput)) #16096

# Merge dataframes by id column since equal and has unique ids. We create a df named df_draft where we do all the working for cleaning and feature engineering
df_draft <- merge(DfCustomerHistory,DfChurnOutput, by = 'id')
write.csv(df_draft, "df_draft.csv")

rm(DfChurnOutput,DfCustomerHistory)

# Get stats of each column
summary(df_draft) # add some analysis regarding consumption and forecasting

```

```{r data_cleaning, include= FALSE}
##########################################
#             Cleaning the Data          #
##########################################

# Number of missing values in each column
na_count <- sapply(df_draft, function(y) sum(length(which(is.na(y)))))
na_count <- data.frame(na_count)

# Number of missing values in percent in each column. 
# We will check which columns have missing values greater than 30% and drop them. We will also ensure that the columns to be dropped dont contain any important information inw hich case we wont drop them.
na_count$na_percent <- sapply(df_draft, function(y) round((sum(length(which(is.na(y))))/length(y))*100.00,2))

# Save the names of columns which have missing values less than 30%
col_30 <- na_count %>% filter(na_percent < 30)
col_30 <- rownames(col_30)

# Remove columns with NA values greater than 30%
df_draft <- df_draft %>% select(all_of(col_30))

# Fill in missing dates with median
## Replace NA values in 'date_end' column with median date
df_draft$date_end[is.na(df_draft$date_end)]<-median(df_draft$date_end,na.rm=TRUE)

##Replace NA values in 'date_renewal' column with median date
df_draft$date_renewal[is.na(df_draft$date_renewal)]<-median(df_draft$date_renewal,na.rm=TRUE)

## Replace NA values in 'date_modif_prod' column with median date
df_draft$date_modif_prod[is.na(df_draft$date_modif_prod)]<-median(df_draft$date_modif_prod,na.rm=TRUE)

## Replace NA values in channel sales column with 'null_channel'
df_draft$channel_sales[is.na(df_draft$channel_sales)] <- "null_channel"

# Convert 'has_gas' column from T/F to 1/0
df_draft <- df_draft %>% mutate('has_gas' =  as.numeric(has_gas))
```

```{r feature_engineering, include= FALSE}
##########################################
#            Feature Engineering         #
##########################################

# Create 'contract duration' column and divide by 365 to get yearly values
df_draft <- df_draft %>% mutate('contract_duration' =  as.integer((difftime(date_end,date_activ, unit = "days"))/(365.25/12)))

# Create reference date for calculations. We will take 1st Jan 2020 since it is given as the reference date
ref_date = ymd(20160101)

# No of years since contract went active
df_draft <- df_draft %>% mutate('months_active' =  as.integer((difftime(ref_date,date_activ, unit = "days"))/(365.25/12)))

# No of years left in contract
df_draft <- df_draft %>% mutate('months_end' =  as.integer((difftime(date_end,ref_date, unit = "days"))/(365.25/12)))

# No of years since last modification at reference date
df_draft <- df_draft %>% mutate('months_modif' =  as.integer((difftime(ref_date,date_modif_prod, unit = "days"))/(365.25/12)))

# Number of months since last renewal at reference date
df_draft <- df_draft %>% mutate('months_renewal' =  as.integer((difftime(ref_date,date_renewal, unit = "days"))/(365.25/12)))

# Remove columns with NA values greater than 30%
df <- df_draft %>% select(all_of(col_30), contract_duration,has_gas,months_active,months_end,months_modif,months_renewal)
df <- df %>% select(-c(date_activ,date_end,date_modif_prod,date_renewal, date_renewal,forecast_cons_year, origin_up))

# Dummy Variable creation for channel_sales and has_gas
## How many dummy columns to make?
# df_backup <- df

## Create dummy variables using the fastdummies library
df <- df %>% dummy_cols(select_columns = c("channel_sales","has_gas"), remove_selected_columns = TRUE)

## Updating column names of dummy variables for easier understanding
df <-  
  rename(df,
    "channel_foos" = channel_sales_foosdfpfkusacimwkcsosbicdxkicaua,
    "channel_usil" = channel_sales_usilxuppasemubllopkaafesmlibmsdf,
    "channel_lmke" = channel_sales_lmkebamcaaclubfxadlmueccxoimlema,
    "channel_ewpa" = channel_sales_ewpakwlliwisiwduibdlfmalxowmwpci,
    "channel_epum" = channel_sales_epumfxlbckeskwekxbiuasklxalciiuu,
    "channel_sddi" = channel_sales_sddiedcslfslkckwlfkdpoeeailfpeds,
    "channel_fixd" = channel_sales_fixdbufsefwooaasfcxdxadsiekoceaa,
    "channel_null" = channel_sales_null_channel
  )

## Remove one of the dummy columns to cater for multicollinearity. We will remove 'channel_null' and 'has_gas_0'
df <- subset(df,select = -c(channel_null,has_gas_0))

data_skim <- skimr::skim(df)
```

The raw data consisted of 33 columns and 16096 observations in each column. The data represents the several characteristics of the company that could play an important part in their churning and retention. The data contains variables such as dates, forecasted consumption and prices, net/gross margins and churn status. It is often said that 80% of the time is spent in data cleaning and processing while just 20% is spent on running the analytics on the clean data. This saying was applicable in our case as well. There were several data quality issues such as missing values, outliers, wrong negative values, multicollinearity, and skewed variables. Our dependent variable would be the binary churn variable while the rest of the variables would act as the independent variables.

## **Exploring Data Analysis**

```{r eda, include= FALSE, warning= FALSE, message = FALSE}
# Churn Rate
churn_rate_barchart <- df %>% 
  select(churn) %>%
  group_by(churn) %>%
  summarise(percentage = n()) %>%
  mutate(Percent = round(100*percentage/sum(percentage),1)) %>%
  mutate(status = ifelse(churn == 1, "Churned", "Retention")) %>%
  ggplot(aes(x = "Companies", y = Percent, fill= factor(status, levels=c("Churned","Retention")))) +
  geom_bar(stat = "identity") + geom_text(aes(label = Percent), position = position_stack(vjust = .5)) +
  labs(x = '', fill = "Status")


# Function to calculate no of months in the duration between a date column and reference date
months_length <- function(column1, column2){
  
  dataframe <- data.frame(column1, column2)
  
  bar_chart <- dataframe %>%
    group_by(column1, column2) %>%
    summarise(months_count = n()) %>%
    mutate(status = ifelse(column2 == 1, "Churned", "Retention")) %>%
    ggplot(aes(x = column1, y = months_count, fill= factor(status))) +
    geom_bar(stat = "identity") + theme_bw() +
    labs(x = NULL, y = NULL,fill = "Status")
  
  return(bar_chart)
}

# Duration of contract
contract_duration_barchart <- months_length(df$contract_duration,df$churn)

# No of months passed from contract active date to reference date
months_active_barchart <- months_length(df$months_active,df$churn)

# No of months left till end date from reference date
months_end_barchart <- months_length(df$months_end,df$churn)

# No of months left till renewal from reference date
months_renewal_barchart <- months_length(df$months_renewal,df$churn)

# No of months since contract was last modified 
months_modif_barchart <- months_length(df$months_modif,df$churn)

##### CONSUMPTION VARIABLES EXPLORATORY ANALYSIS #####

color <- "cyan3"
consumption_eda <- df %>%
  select(cons_12m,cons_gas_12m,cons_last_month,imp_cons) %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) +
  facet_wrap(~key, scales = "free") +
  geom_histogram(fill = color) + theme_bw() +labs(y = NULL, x = NULL)


##### FORECAST VARIABLES EXPLORATORY ANALYSIS #####

forecast_eda <- df %>%
  select(forecast_cons_12m,forecast_discount_energy,forecast_meter_rent_12m,forecast_price_energy_p1,forecast_price_energy_p2,forecast_price_pow_p1) %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) +
  facet_wrap(~key, scales = "free") +
  geom_histogram(fill = color) + theme_bw() +labs(y = NULL, x = NULL)

##### MARGIN VARIABLES EXPLORATORY ANALYSIS #####

margin_eda <- df %>%
  select(margin_gross_pow_ele,margin_net_pow_ele,net_margin) %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) +
  facet_wrap(~key, scales = "free") +
  geom_histogram(fill = color) + theme_bw() +labs(y = NULL, x = NULL)

##### OTHER VARIABLES EXPLORATORY ANALYSIS #####

other_eda <- df %>%
  select(nb_prod_act,num_years_antig,pow_max) %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) +
  facet_wrap(~key, scales = "free") +
  geom_histogram(fill = color) + theme_bw() +labs(y = NULL, x = NULL)

```

```{r loess, include = FALSE, warning = FALSE, message = FALSE}

# Non Parametric Charts
# cons_12m
cons_12m_sc <- ggplot(df , aes(x = cons_12m, y = churn)) +
  geom_point() +
  geom_smooth(method="loess")+
  labs(x = "cons_12m",y = "Churn", title = "Level - Level for churn~cons_12m") 

ln_cons_12m_sc <- ggplot(df , aes(x = cons_12m, y = churn)) +
  geom_point() +
  geom_smooth(method="loess")+
  labs(x = "cons_12m, ln scale",y = "Churn", title = "Level - log for churn~ln_cons_12m") + 
  scale_x_continuous(trans = log_trans())

# cons_last_month
cons_last_month_sc <- ggplot(df , aes(x = cons_last_month, y = churn)) +
  geom_point() +
  geom_smooth(method="loess")+
  labs(x = "cons_last_month",y = "Churn", title = "Level - Level for churn~cons_last_month") 

ln_cons_last_month_sc <- ggplot(df , aes(x = cons_last_month, y = churn)) +
  geom_point() +
  geom_smooth(method="loess")+
  labs(x = "cons_last_month, ln scale",y = "Churn", title = "Level - log for churn~ln_cons_last_month") + 
  scale_x_continuous(trans = log_trans())

# imp_cons
imp_cons_sc <- ggplot(df , aes(x = imp_cons, y = churn)) +
  geom_point() +
  geom_smooth(method="loess") +
  labs(x = "imp_cons",y = "Churn", title = "Level - Level for churn~imp_cons") 

ln_imp_cons_sc <- ggplot(df , aes(x = imp_cons, y = churn)) +
  geom_point() +
  geom_smooth(method="loess")+
  labs(x = "imp_cons, ln scale",y = "Churn", title = "Level - log for churn~ln_imp_cons") + 
  scale_x_continuous(trans = log_trans())

# cons_gas_12m
cons_gas_12m_sc <- ggplot(df , aes(x = cons_gas_12m, y = churn)) +
  geom_point() +
  geom_smooth(method="loess") +
  labs(x = "cons_gas_12m",y = "Churn", title = "Level - Level for churn~cons_gas_12m") 

ln_cons_gas_12m_sc <- ggplot(df , aes(x = cons_gas_12m, y = churn)) +
  geom_point() +
  geom_smooth(method="loess")+
  labs(x = "cons_gas_12m, ln scale",y = "Churn", title = "Level - log for churn~ln_cons_gas_12m") + 
  scale_x_continuous(trans = log_trans())

# forecast_cons_12m
forecast_cons_12m_sc <- ggplot(df , aes(x = forecast_cons_12m, y = churn)) +
  geom_point() +
  geom_smooth(method="loess") +
  labs(x = "forecast_cons_12m",y = "Churn", title = "Level - Level for churn~forecast_cons_12m") 

ln_forecast_cons_12m_sc <- ggplot(df , aes(x = cons_gas_12m, y = churn)) +
  geom_point() +
  geom_smooth(method="loess")+
  labs(x = "forecast_cons_12m, ln scale",y = "Churn", title = "Level - log for churn~ln_forecast_cons_12m") + 
  scale_x_continuous(trans = log_trans())

# forecast_meter_rent_12m
forecast_meter_rent_12m_sc <- ggplot(df , aes(x = forecast_meter_rent_12m, y = churn)) +
  geom_point() +
  geom_smooth(method="loess") +
  labs(x = "forecast_meter_rent_12m",y = "Churn", title = "Level - Level for churn~forecast_meter_rent_12m") 

ln_forecast_meter_rent_12m_sc <- ggplot(df , aes(x = forecast_meter_rent_12m, y = churn)) +
  geom_point() +
  geom_smooth(method="loess")+
  labs(x = "forecast_meter_rent_12m, ln scale",y = "Churn", title = "Level - log for churn~ln_forecast_meter_rent_12m") + 
  scale_x_continuous(trans = log_trans())
```

```{r further_data_cleaning, include = FALSE,warning = FALSE, message = FALSE}

# Checkpoint for further data cleaning
df_ftc <- df
# df <- df_ftc

# Subsetting important variables for outlier treatment
# df <- subset(df,select = -c(id,cons_12m,cons_gas_12m,cons_last_month,imp_cons,forecast_cons_12m,forecast_meter_rent_12m))
df_other <- df %>% select(-c(forecast_price_energy_p1,forecast_price_energy_p2,forecast_price_pow_p1,margin_gross_pow_ele,margin_net_pow_ele,net_margin,pow_max,cons_12m,cons_gas_12m,cons_last_month,imp_cons,forecast_cons_12m,forecast_meter_rent_12m))
df <- df %>% select(c(forecast_price_energy_p1,forecast_price_energy_p2,forecast_price_pow_p1,margin_gross_pow_ele,margin_net_pow_ele,net_margin,pow_max,cons_12m,cons_gas_12m,cons_last_month,imp_cons,forecast_cons_12m,forecast_meter_rent_12m))

# Detecting outliers and replacing them with mean
remove_outliers <- function(column_name){
  outliers <- boxplot(column_name, plot=FALSE)$out
  if(length(outliers) == 0){ column_name  <- column_name} else{
    column_name[column_name %in% outliers] = mean(column_name,na.rm = TRUE); column_name
  }
}

# Application of outlier function
df <- data.frame(sapply(df, remove_outliers))

# Join the treated dataset with rest of variables
df <- cbind(df_other,df)

# Continue on with df dataframe and treat it for missing values
# Remove all NA values and replace with mean

df_id <- df %>% select(id)
df <- df %>% select(-id)

rm(df_other)
```

```{r transformation, include=FALSE, message=FALSE, warning=FALSE}
##########################################
#        Transformation of data          #
##########################################
##### Transformation of Consumption variables #####

# Consumption variables are right skewed as we saw from the histograms. To make them normally distributed, we will
# take log of these variables. However, these 4 variables include negative and zero values for which log cant be taken.So 
# we will convert negative values to NaN and add a constant 1 to these variables as well

# Set negative values as Na as log cant be taken for negative values
df <- df %>% mutate(cons_12m = replace(cons_12m, which(cons_12m < 0), NA))
df <- df %>% mutate(cons_gas_12m = replace(cons_gas_12m, which(cons_gas_12m < 0), NA))
df <- df %>% mutate(cons_last_month = replace(cons_last_month, which(cons_last_month < 0), NA))
df <- df %>% mutate(imp_cons = replace(imp_cons , which(imp_cons  < 0), NA))

# Add constant 1 to the variables and then take log since some values are zero
df <- df %>% mutate( ln_cons_12m = log( cons_12m + 1 ),
                     ln_cons_gas_12m = log( cons_gas_12m + 1),
                     ln_cons_last_month = log(cons_last_month + 1),
                     ln_imp_cons = log(imp_cons + 1)) 


##### Transformation of Forecast variables #####

# Set negative values as NaN as log cant be taken for negative values
df <- df %>% mutate(forecast_cons_12m = replace(forecast_cons_12m, which(forecast_cons_12m < 0), NA))
df <- df %>% mutate(forecast_meter_rent_12m = replace(forecast_meter_rent_12m, which(forecast_meter_rent_12m < 0), NA))


# Add constant 1 to the variables and then take log
df <- df %>% mutate( ln_forecast_cons_12m = log(forecast_cons_12m + 1),
                     ln_forecast_meter_rent_12m = log(forecast_meter_rent_12m + 1)) 

# Fill missing values with mean
df <- data.frame(sapply(df, function(x) replace(x, is.na(x), mean(x, na.rm = TRUE))))
# backup2 <- df

# Now check for the distribution of the transformed variables
transformed_eda <- df %>%
  select(ln_cons_12m,ln_cons_gas_12m,ln_cons_last_month,ln_imp_cons,ln_forecast_cons_12m,ln_forecast_meter_rent_12m) %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) +
  facet_wrap(~key, scales = "free") +
  geom_histogram(fill = color) + theme_bw() +labs(y = NULL, x = NULL)

# Remove original columns which have been transformed
df <- df %>% select(-c(cons_12m,cons_gas_12m,cons_last_month,imp_cons,forecast_cons_12m,forecast_meter_rent_12m))


```

```{r correlation_matrix, include = FALSE, warning = FALSE, message = FALSE}

##### Correlation Matrix #####
cor1 <- cor(df, use = "pairwise.complete.obs")
cor_matrix <- ggcorrplot::ggcorrplot(cor1, method = "square",lab = TRUE, type = "lower", lab_size = 2.5, digits = 2,ggtheme = theme_bw)

# From the correlation matrix, we can see that 'contract_duration'& 'month_activ','num_years_antig' & 'months_end', 'margin_gross_power_ele' & 'margin_net_power_ele' have the highest correlation
# Calculate Variance Inflation Factor using the 'car' package
lm_model <- lm(churn ~ ., data = df)
vif <- data.frame(car::vif(lm_model))
vif <- rename(vif, VIF = car..vif.lm_model.)

# From the VIF we can confirm that the correlations are very high and we can drop one of the variables from each correlation pair.

df <- subset(df, select = -c(contract_duration,num_years_antig, margin_gross_pow_ele,ln_cons_gas_12m))

```

The missing data (**Table 1**) was very small for majority of the variables, so we were able to easily replace the missing values with mean and median approximations.Any column that had more than 30% missing observations were removed. We then conducted Feature Engineering. We converted dates into durations to make them more useful. After getting done with our initial data cleaning and feature Engineering, we checked the distributions of the variables through histograms (**Figure 1-7**).

Our exploratory data analysis shows that around 10% of the of total customers have churned (**Figure 1**). All Consumption related variables (cons\_12m ,cons\_last\_month,imp\_cons,cons\_gas\_12m ) and 2 of the forecast variables (forecast\_cons\_12m,forecast\_meter\_rent\_12m) are extremely right skewed (**Figure 2,3**). The values on the higher end and lower ends of the distribution are potentially outliers. Next we checked a non-parametric estimator loess smoother to have a general idea about the functional form between a variable and churn. We will specifically focus on those variables who had skewed distributions (Figure ). As expected, the functional forms are very non linear and hence we took log of the 6 variables( 4 consumption and 2 forecast variables) to give the loess line a more linear shape (**Figure 7**). The transformation of the log variables can be seen in (**Figure 8**).

We used boxplots to tell us more about the outliers and what their values are. We then removed these values and replaced them with the mean of the respective columns values excluding the outliers.

**Multicollinearity and Dummy Variables**

When you have two independent variables that are very highly correlated, you definitely should remove one of them because you run into the multicollinearity conundrum and your regression model's regression coefficients related to the two highly correlated variables will be unreliable. We checked for multicollinearity between the explanatory variables. We drew a correlated matrix (**Figure 9**) to observe which pair of variables were highly correlated. Once we found the pairs, we calculated the Variance Inflation Factor (VIF) (**Table 2**) to confirm the correlation and remove one variable from the pairs.This was done to reduce multicollinearity. For this, we first created the correlation matrix. For pairs that have very high correlations, we will use Variance Inflation Factor to determine which variable to remove. Categorical columns ('channel\_sales' and 'has\_gas') were converted into dummy columns and the reference columns were removed.

**Machine Learning and Predictions**

```{r split_data, incLude = FALSE, warning= FALSE, message = FALSE}
df$churn <- as.factor(df$churn)

##### SPLIT DATA INTO TEST/TRAIN DATASET #####
set.seed(1111)
intrain<- createDataPartition(df$churn,p=0.75,list=FALSE)
train <- df[intrain,]
test <- df[-intrain,]

```

```{r lpm, incLude = FALSE, warning= FALSE, message = FALSE}

##########################################
#         Linear Probability Model       #
##########################################

train_lpm <- test
test_lpm <- test

train_lpm$churn <- as.numeric(train_lpm$churn)
test_lpm$churn <- as.numeric(test_lpm$churn)

##### SPLIT DATA INTO TEST/TRAIN DATASET #####

lp_model <-lm(churn ~ ., data = train_lpm)
summary(lp_model)

# Save the model output
 lp_table <- huxtable::huxreg(lp_model)

# Check predicted probabilities: is there any interesting values?
# predicted probabilities
test$pred_lpm <- predict(lp_model,test_lpm )
# Make a descriptive summary of the predictions!
summary(pred_lpm )

# Show the predicted probabilities' distribution (ggplot)
lpm_pred_prob <- ggplot( test , aes( x = pred_lpm ) ) +
  geom_histogram(fill = 'cyan3', color = 'black') + theme_bw() + labs(title = "Predicted Probabilities -LPM Model")

test_lpm <- test_lpm %>% 
  mutate( q100_pred_lpm = ntile(pred_lpm, 100) )

# Make a summary statistics, using sum_stat for the bottom (q100_pred_lpm==1) 
#   and top 1% (q100_pred_lpm==100), using stats = c('mean','median','sd')
#   and variables c('smoking','ever_smoked','female','age','eduyears','income10','bmi','exerc')
#   use the num_obs = F input for sum_stat

# Top 1%
t1 <- source('https://raw.githubusercontent.com/CEU-Economics-and-Business/ECBS-5208-Coding-1-Business-Analytics/master/Class_10/codes/sum_stat.R')( subset( share , q100_pred_lpm==100 ) , 
                c('forecast_discount_energy','nb_prod_act','months_active   ','months_end','months_modif','months_renewal','channel_epum','channel_ewpa','channel_foos','channel_lmke','channel_sddi','channel_usil','has_gas_1','forecast_price_energy_p1','forecast_price_energy_p2','forecast_price_pow_p1','margin_net_pow_ele','net_margin','pow_max','ln_cons_12m','ln_cons_last_month','ln_imp_cons','ln_forecast_cons_12m','ln_forecast_meter_rent_12m' ),
                c('mean','median','sd'),
                num_obs = F )
t1

# Bottom 1%
b1 <- source('https://raw.githubusercontent.com/CEU-Economics-and-Business/ECBS-5208-Coding-1-Business-Analytics/master/Class_10/codes/sum_stat.R')( subset( share , q100_pred_lpm==1 ) , 
                c('forecast_discount_energy','nb_prod_act','months_active   ','months_end','months_modif','months_renewal','channel_epum','channel_ewpa','channel_foos','channel_lmke','channel_sddi','channel_usil','has_gas_1','forecast_price_energy_p1','forecast_price_energy_p2','forecast_price_pow_p1','margin_net_pow_ele','net_margin','pow_max','ln_cons_12m','ln_cons_last_month','ln_imp_cons','ln_forecast_cons_12m','ln_forecast_meter_rent_12m' ),
                c('mean','median','sd'),
                num_obs = F )
b1



```

```{r glm, include = FALSE, warning= FALSE, message = FALSE}

#########################################
##### LOGISTIC REGRESSION VIA GLM()  ####
#########################################

glm_train <- train
glm_test <- test

# Logit Model 1
logit_model1 <- glm(as.factor(churn) ~ ., data = glm_train, family = binomial("logit"))

# Logit Model 2
logit_model2 <- glm(as.factor(churn)~. -channel_sddi -channel_epum -net_margin -ln_forecast_meter_rent_12m -nb_prod_act -forecast_price_pow_p1 -months_end, data = glm_train, family = binomial("logit"))

# Probit Model 1
probit_model <- glm(as.factor(churn)~., data = glm_train, family = binomial("probit"))


# Model  Prediction with Logit
pred_type_test_l <- predict(logit_model2, newdata = glm_test, type = "response")
pred_type_logit <- pred_type_test_l
glm_test <- cbind(pred_type_test_l,test)
pred_type_test_l <- ifelse(pred_type_test_l > 0.5, 1, 0)

# Marginal Differences Logit
# Calculate logit marginal differences
logit_marg <- logitmfx(as.factor(churn)~., data=glm_train, atmean=FALSE, robust = T)
print(logit_marg)

#Model Prediction with Probit
pred_type_logit  <- predict(glm_model1, newdata = glm_test, type = "response")
glm_test <- cbind(pred_type_logit,test)

# Calculate probit marginal differences
probit_marg <- probitmfx(as.factor(churn)~., data=glm_train, atmean=FALSE, robust = T)
print( probit_marg )

#Save models
w_dir <- 'C:/Users/abc/OneDrive/Business_Analytics/Data-Analysis-1-2-3/Data_Analysis_2/DA2_final_project/out/'
cm <- c('(Intercept)' = 'Constant')
pmodels <- list(logit_model1, logit_model2,logit_marg, probit_model, probit_marg)
msummary( pmodels ,
          fmt="%.3f",
          gof_omit = 'DF|Deviance|Log.Lik.|F|R2 Adj.|AIC|BIC|R2|PseudoR2',
          stars=c('*' = .05, '**' = .01),
          coef_rename = cm,
          coef_omit = 'as.factor(churn)*',
          output = paste0(w_dir,"prob_models_coeff.html")
)
?msummary


# Confusion Matrix
cm_glm <- confusionMatrix(as.factor(pred_type_test_l),as.factor(glm_test$churn))


# Producing ROC curve of model
#pred_response <- predict(glm_model1, newdata = test, type = "response")
predictFull <- prediction(pred_type_test_l,as.factor(glm_test$churn))

# Plot AUC
auc_roc <- performance(predictFull, measure = 'tpr',x.measure = 'fpr')
plot(auc_roc, col = "blue")

auc_score_glm <- auc(glm_test$churn, pred_type_test_l) #0.6221063
rmse_score_glm <- rmse(glm_test$churn, pred_type_test_l) # 0.2994171
```



We used 3 Machine Learning models to predict whether a customer would churn or not and their probabilities.

**Logistic Regression**:

Logistic Regression belongs to the family of generalized linear models. It is a binary classification algorithm used when the response variable is dichotomous (1 or 0). Inherently, it returns the set of probabilities of target class. But, we can also obtain response labels using a probability threshold value.

For our analysis, we first split the data 75/25 into train and test models respectively. We ran the Logistic Regression via the ML Caret Package. We used K fold Cross Validation on our training set in order to flag problems like overfitting or selection bias and to give an insight on how the model will generalize to an independent dataset. For further robustness check, we ran 3 variants of the Logistic Regression model with varying \$\$B coefficients.

The equation for our first model is:

$$
churn = \beta_0+ \beta_1cons\_12m_i + \beta_2log(cons\_last\_month)_i +\beta_3log(imp\_cons)_i+ \beta_4forecast\_price\_energy\_p1_i + \\ \beta_5forecast\_price\_energy\_p2_i + \beta_6forecast\_price\_pow\_p1_i + \beta_7forecast\_discount\_energy_i + \beta_8log(forecast\_meter\_rent\_12m_i) +\\ \beta_9months\_active_1 + \beta_{10}months\_modif_i + \beta_{11}months\_renewal_i + \beta_{12}channel\_usil_i + \beta_{13}channel\_lmke_i +\beta_{14}channel\_ewpa_i + \\ \beta_{15}channel\_foos_i + \beta_{16}channel\_sddi_i + \beta_{17}channel\_epum_i + \beta_{18}\_months\_end+i + \beta_{19}b\_prod\_act_i + \beta_{20}\_pow\_max_i \\ + \beta_{21}\_has\_gas\_1_i + \beta_{22}\_net\_margin_i +\beta_{23}margin\_net\_pow\_ele_i + \beta_{24}log(forecast\_cons\_12m_i)
$$

We ran the logistic regression with all the explanatory variables and received the results as shown in

```{r xgboost, include = FALSE, message = FALSE, warning = FALSE}

set.seed(2018)
intrain<- createDataPartition(df_xgb$churn,p=0.75,list=FALSE)
ml_train <- df[intrain,]
ml_test <- df[-intrain,]

labels <- ml_train$churn
ts_label <- ml_test$churn

labels <- as.numeric(as.character(labels))
ts_label <- as.numeric(as.character(ts_label))

#XGBoost takes matrix for data hence we convert dataframe to matrix
# df_xgb_backup<- df_xgb
xgb_train <- ml_train %>% select(-churn)
xgb_train <- xgb.DMatrix(data = as.matrix(xgb_train),label = labels)

xgb_test <- ml_test %>% select(-churn)
xgb_test <- xgb.DMatrix(data = as.matrix(xgb_test),label = ts_label)

#default parameters
xgb_params <- list(booster = "gbtree", 
                   objective = "binary:logistic", 
                   eta=0.3, gamma=0, 
                   max_depth=6, 
                   min_child_weight=1, 
                   subsample=1, 
                   colsample_bytree=1)

# Calculate the best nround for this model. In addition, this function also returns CV error, which is an estimate of test error.
set.seed(2018)
xgbcv <- xgb.cv( params = xgb_params, data = xgb_train, nrounds = 500, nfold = 5, showsd = T, stratified = T, print_every_n = 10, early_stopping_rounds = 20, maximize = F, missing = NA)

# Iteration 47 gave lowest test error

elog <- as.data.frame(xgbcv$evaluation_log)
nround <- which.min(elog$test_error_mean)
##best iteration = 47
## The model returned lowest error at the 47th (nround) iteration.
# CV accuracy is 1-0.0968 = 90.32%

#first default - model training
xgb_model <- xgb.train(params = xgb_params, 
                        data = xgb_train, 
                        nrounds = nround, 
                        watchlist = list(train=xgb_train,test=xgb_test), 
                        print_every_n = 10, early_stop_round = 10, 
                        maximize = F , 
                        eval_metric = "error")

summary(xgb_model)
#model prediction
xgbpred <- predict(xgb_model,xgb_test)

summary(xgbpred)

# The objective function binary:logistic returns output probabilities rather than labels. To convert it, we need to 
# manually use a cutoff value. As seen above, I've used 0.5 as my cutoff value for predictions. We can calculate our model's
# accuracy using confusionMatrix() function from caret package.

#confusion matrix
xgbpred <- ifelse(xgbpred > 0.5,1,0)
cm_xgb <- confusionMatrix (as.factor(xgbpred), as.factor(ts_label))
#Accuracy - 86.54%` 

#view variable importance plot
mat <- xgb.importance (feature_names = colnames(df_xgb),model = xgb_model)

# The ggplot-backend method also performs 1-D clustering of the importance values, with bar colors 
# corresponding to different clusters that have somewhat similar importance values.
xgb_feature_plot <- xgb.ggplot.importance (importance_matrix = mat)

library(Metrics) 
roc_test <- roc(ts_label, xgbpred, algorithm = 2)
roc_xgb <- plot(roc_test) 
auc_xgb <- auc(ts_label, xgbpred) #0.5684948
rmse_xgb <- rmse(ts_label, xgbpred) # 0.2953
```

\newpage

## **APPENDIX A**

\newpage

## **APPENDIX B**

**Table 1 - Missing Values**

```{r missing_values, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(na_count, caption = "Missing Values in each Variable")
```

\newpage

**Table 2 - Variance Inflation Factor (VIF)**

```{r vif,echo=FALSE, warning = FALSE, message = FALSE}
```

```{r vif,echo=FALSE, warning = FALSE, message = FALSE}
kable(vif, caption = "Variance Inflation Factor (VIF) ")
```

\newpage

## **APPENDIX C**

\newpage

```{r, plot_intro, echo = FALSE, warning= FALSE,message= FALSE, fig.width=8,fig.height=4, fig.align="center"}
plot_intro_bar + labs(title = "Data Overview")
```

**Figure: 1 - Churn Rate**

```{r churn_rate, echo = FALSE, warning= FALSE,message= FALSE, fig.width=8,fig.height=4, fig.align="center"}
print(churn_rate_barchart)
```

**Figure: 2 - Month Duration charts**

```{r months_active_end, echo=FALSE, fig.height=4, fig.width=8, message=FALSE, warning=FALSE, fig.align="center"}
ggarrange1 <- ggarrange(months_active_barchart,months_end_barchart,months_renewal_barchart,months_modif_barchart,
          nrow = 2, ncol = 2,
          common.legend = TRUE,
          labels = c("Months Active", "Months End", "Months Renewal", "Months_modif"),
          vjust = c(-0.5,-0.5,0.4,0.4),
          hjust = c(-1.8,-2.4,-1.5,-2.0),
          font.label = list(size = 10))

annotate_figure(ggarrange1,
                bottom = text_grob("No of Months", color = "black",
                                   hjust = 4.3, x = 1, size = 11, face = "bold"),
                left = text_grob("No of Companies", color = "black", rot = 90, face = "bold",size = 11))
```

\newpage

**Figure: 3 - Consumption Variables Exploration**

```{r consumption_eda, echo = FALSE, warning = FALSE, message = FALSE, fig.align="center"}
##### CONSUMPTION VARIABLES EXPLORATORY ANALYSIS #####
annotate_figure(consumption_eda,
                bottom = text_grob("Consumption", color = "black",
                                   hjust =3.9, x = 1, size = 11, face = "bold"),
                left = text_grob("No of Companies", color = "black", rot = 90, face = "bold",size = 11))


```

**Figure: 4 - Forecast Variables Exploration**

```{r forecast_eda, echo=FALSE, message=FALSE, warning=FALSE, fig.align="center",fig.height= 5}
annotate_figure(forecast_eda,
                bottom = text_grob("Dollars ($)", color = "black",
                                   hjust = 5, x = 1, size = 11, face = "bold"),
                left = text_grob("No of Companies", color = "black", rot = 90, face = "bold",size = 11))
```

\newpage

**Figure: 5 - Margin Variables Exploration**

```{r margin_eda, echo = FALSE, message=FALSE, warning=FALSE, fig.align="center"}
annotate_figure(margin_eda,
                bottom = text_grob("Dollars ($)", color = "black",
                                   hjust = 4.75, x = 1, size = 11, face = "bold"),
                left = text_grob("No of Companies", color = "black", rot = 90, face = "bold",size = 11))
```

**Figure: 6 - Other Variables Exploration**

```{r other_eda, echo = FALSE, message=FALSE, warning=FALSE, fig.align="center"}
annotate_figure(other_eda,
                bottom = text_grob("Value", color = "black",
                                   hjust = 8.8, x = 1, size = 11, face = "bold"),
                left = text_grob("No of Companies", color = "black", rot = 90, face = "bold",size = 11))
```

**Figure: 7 - Loess/Scatterplot of Variables**

```{r, echo = FALSE, fig.align="center", message=FALSE, warning=FALSE, echo=FALSE}
ggpubr::ggarrange(cons_12m_sc,ln_cons_12m_sc, nrow = 1, ncol = 2)
ggpubr::ggarrange(cons_last_month_sc,ln_cons_last_month_sc, nrow = 1, ncol = 2)
ggpubr::ggarrange(imp_cons_sc,ln_imp_cons_sc, nrow = 1, ncol = 2)
ggpubr::ggarrange(cons_gas_12m_sc,ln_cons_gas_12m_sc, nrow = 1, ncol = 2)
ggpubr::ggarrange(forecast_cons_12m_sc,ln_forecast_cons_12m_sc, nrow = 1, ncol = 2)
 


#ln_cons_last_month_sc,imp_cons_sc,ln_imp_cons_sc,cons_gas_12m_sc,ln_cons_gas_12m_sc,forecast_cons_12m_sc, ln_forecast_cons_12m_sc,forecast_meter_rent_12m_sc,ln_forecast_meter_rent_12m_sc, nrow = 6, ncol = 2)

```

**Figure: 8 - Transformed Variables Exploration**

```{r, echo = FALSE, warning = FALSE, message = FALSE, fig.align="center"}
annotate_figure(transformed_eda,
                bottom = text_grob("Value", color = "black",
                                   hjust = 8.8, x = 1, size = 11, face = "bold"),
                left = text_grob("No of Companies", color = "black", rot = 90, face = "bold",size = 11))
```

**Figure 9 - Correlation Matrix**

```{r correlation_matrix_chart,echo = FALSE, warning = FALSE, message = FALSE, fig.height=10,fig.width=10, fig.align="center"}
cor_matrix + rotate_x_text(angle = 90) + font("xlab", size = 1, color = "black")+
 font("ylab", size = 1, color = "black")

```
